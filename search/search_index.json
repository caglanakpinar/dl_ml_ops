{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deep Learning ML OPS","text":"<p>Deep Learning ML OPS, known as <code>dl-ml-ops</code>, is a kind of a ml tool to make your life easier in order to use neural network only with .yaml files. <code>dl-ml-ops</code> helps you to build network with multiple towers, multi-layer-perceptrons without touching any code, it allows us to train with given parameters.  It also allows us to run hyperparameter tuning and serve and monitor the model performance metrics. </p> <p>Tool is uses open-source deep learning tools tensorflow, keras, keras_tunes in background. </p>"},{"location":"#installation","title":"Installation","text":"<p>Tool can be used any other package by install it via git command</p> <pre><code>poetry add git+https://github.com/caglanakpinar/dl_ml_ops.git\n</code></pre> <ul> <li><code>poetry run main.py model train --training_class</code> - run model</li> <li><code>poetry run main.py model tune --tuning_class</code> - run model</li> <li><code>poetry run main.py model serve --serving_class</code> - run model [DEV]</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mlp/\n    cli/   \n        - cli.py  \n    configs/\n        - configurations.py  \n    data_access/  \n        - base.py\n    logger/\n        - logs.py\n    monitoring/  # not yet implemented\n    serve/       # not yet implemented\n    train/\n        - base.py\n        - builder.py\n        - models.py\n        - trainer.py\n        - tuner.py\n    utils/\n        - paths.py\n</code></pre>"},{"location":"build/","title":"build Network","text":"<p>Before training or tuning process, you need to create network structure by using tensorflow*-keras.  Either you can pass your custom model to the framework, or, let it create for you k-only by passing configuration .yaml</p>"},{"location":"build/#build-trainer","title":"Build Trainer","text":""},{"location":"build/#build-trainer-from-configurations","title":"Build Trainer from Configurations","text":""},{"location":"build/#build-custom-trainer","title":"Build Custom Trainer","text":""},{"location":"build/#build-tuner","title":"Build Tuner","text":""},{"location":"build/#build-tuner-from-configurations","title":"Build Tuner from Configurations","text":""},{"location":"build/#build-custom-tuner","title":"Build Custom Tuner","text":""},{"location":"data_access/","title":"Data Access","text":"<p>To be able to access a data <code>mlp.BaseData</code> will be used for class. model class will be pass to cli to run data access. In this data class data will be generated and pass into the trainer or tuner.</p> <p>argument for data access is</p> <p><pre><code>--data_class ...\n</code></pre> If dataset need validation - train split, only add <code>split_ratio</code> field to .yaml at <code>--trainer_config_path</code>. Number of inputs (features), which is dimensions of input, has to be added as field as <code>input_size</code> at <code>trainer_config_path</code>. Number of output dimensions has to be added to <code>trainer_config_path</code> as a field as <code>output_size</code> (integer).  Data Access will only being used in training and tuning processes.</p>"},{"location":"data_access/#how-to-create-data-access-class-by-using-mlpbasedata","title":"How to create Data Access class by using <code>mlp.BaseData</code>","text":"<p>once you added <code>--data_class</code> argument in terminal framework will detect classes and will be executed. Make sure <code>mlp.BaseData</code> will be used as sub-class. you can do whatever you want in <code>data_class</code>.  At this data access class, there 2 abstract methods has to be added to <code>read</code>and <code>__init__</code>. Both will take params as argument. Framework needs <code>read</code> function. <code>read</code> function always takes <code>params</code>. It will be executed as <code>classmethod</code> and will return your data access class Within <code>read</code> function, your data access clas will be created. Data will be fetched regarding your data source (wherever from). this fetched dataset has to be assigned <code>data</code> attribute into your data access class Here is an example of how you can build your own data access class. </p> <pre><code>from mlp import BaseData, Params\n\n\nclass MyDataAccess(BaseData)\n    def __init__(self, params: Params):\n        self.params = params\n        ...\n        ...\n\n     @classmethod\n     def read(cls, params: Params):\n        _cls = MyDataAccess(params)\n        ....\n        ....\n        _cls.data = &lt;dont forget to assign raw data to &lt;data&gt; attribute&gt;\n        return _cls\n</code></pre>"},{"location":"data_access/#example-of-creating-data-access-class","title":"Example of creating Data Access class","text":"<p>Here an example of treading data from <code>https://storage.googleapis.com/tf-datasets/titanic/train.csv</code> which is open-source dataset as titanic survivors. We would like to read this data as pandas data fram in our data access class</p> <pre><code>class MyBinaryClassificationData(BaseData):\n    \"\"\"\n    we will be using titanic csv file from\n    https://storage.googleapis.com/tf-datasets/titanic/train.csv\n    \"\"\"\n\n    def __init__(self, params: Params):\n        self.params = params\n\n    @classmethod\n    def read(cls, params: Params, **kwargs):\n        titanic_file = keras.utils.get_file(\"train.csv\", params.get(\"data_url\"))\n        _cls = MyBinaryClassificationData(params)\n        _cls.data = pd.read_csv(titanic_file)\n        log(log.info, \"One Hot Encoding for Categorical Features ...\")\n        for categorical_column in [\n            \"sex\",\n            \"n_siblings_spouses\",\n            \"parch\",\n            \"class\",\n            \"deck\",\n            \"embark_town\",\n        ]:\n            _dummies = pd.get_dummies(\n                _cls.data[categorical_column], dtype=int, prefix=categorical_column\n            )\n            _cls.data = _cls.data.drop(categorical_column, axis=1)\n            _cls.data = pd.concat([_cls.data, _dummies], axis=1)\n        _cls.data[\"alone\"] = _cls.data.alone.apply(lambda x: 0 if x == \"n\" else 1)\n        return _cls\n</code></pre>"},{"location":"intro/","title":"welcome to dl-ml-ops 101","text":"<p>Welcome to DL-ML-OPs world. Before we start, I just wanna make it clear that this platform is not a tool that you don't have any idea about the parameters about Deep Learning and make it within 10 minutes. However, this will help you to implement Neural Network smoothly.  However, there are still need to be carefully and to learn while implementing it.</p>"},{"location":"intro/#what-is-this-all-about","title":"what is this all about?","text":"<p>This is about to build a pipeline which is starting from fetching data, build network with parameters, train tune parameters and serve</p>"},{"location":"intro/#why-do-need-dl-ml-ops","title":"Why do need <code>dl-ml-ops</code>?","text":"<p>recent years, I have been working with networks. everytime I start building it I realized that there should be some wasy to make it more generic that it can make it my life easier and I can operate it from a config file.</p>"},{"location":"intro/#step-by-step-instruction-to-use","title":"Step by Step Instruction to Use","text":"<p>there are 8 sections;  - configurations  - data access  - build network  - train  - store  - hyper parameter tuning  - serve  - monitoring Each section will be explained with example that are stored in <code>example.py</code></p>"},{"location":"intro/#how-to-run","title":"How to run","text":"<p>all comments are executed by <code>click.cli</code>. you need to call <code>mlp.cli.cli</code> to run <code>model.train</code> or <code>model tune</code>.</p>"},{"location":"intro/#train","title":"Train","text":"<p>you can train your Deep Learning model with a .yaml file. you can write your model class by using <code>mlp.BaseModel</code> as subclass. In order to run training; <pre><code>poetry run python ....py model train --training_class ... --trainer_config_path ...\n</code></pre></p>"},{"location":"intro/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>you can tune your model by taking list of hyperparameters from a .yaml and overwrite to a train. yaml. In order to run tuning;</p> <pre><code>poetry run python ....py model tune --tuning_class ... --hyherparameter_config_path ...\n</code></pre>"},{"location":"intro/#data-access","title":"Data Access","text":"<p>To be able to access a data <code>mlp.BaseData</code> will be used for class. model class will be pass to cli to run data access. In this data class data will be generated and pass into the trainer or tuner.</p> <p>argument for data access is</p> <p><pre><code>--data_class ...\n</code></pre> If dataset need validation - train split, only add <code>split_ratio</code> field to .yaml at <code>--trainer_config_path</code>. Number of inputs (features), which is dimensions of input, has to be added as field as <code>input_size</code> at <code>trainer_config_path</code>. Number of output dimensions has to be added to <code>trainer_config_path</code> as a field as <code>output_size</code> (integer).</p>"},{"location":"intro/#configurations","title":"Configurations","text":"<p>Tool only supports .yaml file as config files. there will to 2 kind of YAML file 1st for training and its folder path can be pass as arguments from <code>--trainer_config_path</code>. 2nd is for hyperparameter tuning. its folder path can be pass with argument <code>--tuner_config_path</code>.</p> <p>there are mandatory fields for training yaml;  - name - input_size - output_size - target - split_ratio</p> <p>While tuning process is triggered, first,  it takes list of parameters from <code>trainer_config_path</code>.  Then, if any fields are matching at <code>hyoerparameters_config_path</code>, it takes those as tuning parameters. Those parameters at <code>hyperparameter_config_path</code> must be list in order to run trials for tuning.</p>"},{"location":"intro/#monitoring-dev","title":"monitoring [DEV]","text":"<p>not yet implemented</p>"},{"location":"intro/#serving-dev","title":"Serving [DEV]","text":"<p>not yet implemented</p>"},{"location":"params/","title":"Configurations","text":"<p>Tool is fully configurable from argument that has been passed from terminal or configurable .yaml files. However, there are still rules and mandatory fields to be carefully while designing a pipeline.</p> <p>Tool only supports .yaml file as config files.  there will be 2 kind of YAML files. 1st for training and it is the  folder path can be pass as arguments from <code>--trainer_config_path</code>. 2nd is for hyperparameter tuning. it is the folder path can be pass with argument <code>--tuner_config_path</code>.</p> <p>If extra fields are passed from the terminal, still, it can be used within framework which will be stored in to configuration class <code>Params</code>.</p>"},{"location":"params/#params-object","title":"<code>Params</code> object","text":"<p>This takes all fields from .yaml file and adds as a new attribute into the <code>Params</code>.  When it is needed to call an attribute from it, <code>params.get('&lt;name of the field&gt;')</code> There are 2 types of .yaml can be sorted into the params classes; </p> <ul> <li>trainer configurations; from <code>--trainer_config_path</code></li> <li>tuner configurations; from <code>--tuner_config_path</code></li> </ul> <p>If extra fields are passed from the terminal still it can be used within frame which will be stored in to configuration class <code>Params</code>. let's say we throw below arguments from terminal;</p> <p><pre><code>--l1_regulization_term 0.001 --l2_regulization_term 0.001\n</code></pre> these arguments <code>l1_regulization_term</code> and <code>l2_regulization_term</code> are not used as default arguments while training or tuning. However, it can be passed to <code>Params</code> class from cli from below code;</p> <p><pre><code>cli.py\n....\n\n@click.option(\"--schedule\", default=None)\ndef train(\n    training_class,\n    trainer_config_path,\n    data_access_class,\n    continuous_training,\n    build_network_from_config,\n    schedule,\n    **kwargs,\n):\n    ...\n    ....\n\n    params = Params(\n        trainer_config_path,\n        **{\n            **kwargs,\n            **{\n                \"continuous_training\": continuous_training,\n                \"build_network_from_config\": build_network_from_config,\n            },\n        },\n    )\n</code></pre> all arguments from terminal will be captured and available on <code>kwargs</code> on cli. </p>"},{"location":"params/#specific-training-arguments","title":"Specific Training Arguments","text":"<p>Above fields are mandatory depending on your model architect. </p> <pre><code>name: \"give a name to model this will be used anywehere such as chekpoint save, etc\"\ninput_size: \"mandotary when --build_network_from_config True\"\noutput_size: \"mandotary when --build_network_from_config True\"\ntarget: \"mandotary when --build_network_from_config True and it is supervised\"\nsplit_ratio: \"mandotary\"\nmetrics: \"metrics that will be used for \"\n</code></pre>"},{"location":"params/#specific-tuning-arguments","title":"Specific tuning Arguments","text":"<p>Above fields are mandatory depending on your model architect. </p> <pre><code>max_trials: \"number of maximum trials for tune parameterss\"\n</code></pre>"},{"location":"params/#example-training-configurations","title":"Example Training Configurations","text":"<p>example for training .yaml file  <pre><code>activation: relu\nactivation_output: sigmoid\nbatch_size: 64\ncheckpoint_monitor: accuracy\ncheckpoint_save_frequency: 5\ndata_url: https://storage.googleapis.com/tf-datasets/titanic/train.csv\ndropout: 0.0\nepochs: 10\nh_layers: 3\ninput_size: 33\nl1: 0.0001\nl2: 0.0001\nloss: binary_crossentropy\nlr: 0.0004\nmetrics:\n- accuracy\nname: BINARY_CLASSIFICATION\noptimizer: adam\noutput_size: 1\ntarget: alone\nunits: 16\nuse_bias: false\n</code></pre></p> <p>each fields above will be added to <code>Params</code> class.</p>"},{"location":"params/#example-of-tuning-configurations","title":"Example of Tuning Configurations","text":"<p>fields are in tuning .yaml file has to be found in training .yaml file. each field of values must return list. each trial selected value from the list will be used for tuning.</p> <pre><code>lr:\n  - 0.0001\n  - 0.0002\n  - 0.0003\n  - 0.0004\n  - 0.0005\nmax_trials: 5\nh_layers:\n  - 5\n  - 4\n  - 3\n</code></pre> <p><code>lr</code> and <code>h_layers</code> will be tuned regarding of given list of values.</p>"},{"location":"run/","title":"How to Run","text":"<p>1st, you need to create a <code>.py</code> and here is the template of it;</p> <pre><code>from mlp.cli.cli import cli\n\n....\n\n\nif __name__ == \"__main__\":\n    cli()\n</code></pre> <p>2nd, you will create training configuration yaml. path of training .yaml  will be passed from the terminal argument <code>--trainer_config_path</code>. For more details, take a look at hot to create configuration .yaml file</p> <p>3rd, you need data access. To access and pass your data to tool, you need to create data access class.   how to create data access will guide about  creating datasets for training and tuning. class name and if there is a module name will passed from terminal argument <code>--data_access_class</code>.</p> <p>4th, model will be ready to train after creating model training class with using <code>mlp.BaseModel</code>.  by passing created class to the <code>--training_class</code>, framework will start training process. for easy way, you can directly use training configuration  without using <code>--training_class</code>.  To use default network builder from <code>mlp</code> tool, <code>--build_network_from_config True</code> has to be True. for more details pls take a look how to build network  and for more details about training network, take a look at how to train your network</p> <p>Optional, you can tune your model. After tuning process is done, parameters will be updated on trainer configuration .yaml file. First, you will create tuner configuration yaml. Path of tuner .yaml will be passed from the terminal argument <code>--tuner_config_path</code>. For more details, take a look at hot to create configuration .yaml file.  After that, model will be ready to tune after creating model tuner class with using <code>mlp.BaseHyperModel</code>. By passing created class to the <code>--tuning_class</code>, framework will start tuning process. for easy way, you can directly use tuning configuration  without using <code>--tuning_class</code>.  To use default hyperparameter tuner from <code>mlp</code> tool, <code>--build_network_from_config True</code> has to be True. for more details pls take a look how to build network  and for more details about tuner, take a look at how to tune your network</p>"},{"location":"train/","title":"Train","text":"<p>Training process will be executed by the Tool after given arguments from the terminal. In order to execute training process</p>"},{"location":"train/#trainer-class","title":"Trainer Class","text":"<p>You can directly use your own Network to train when you will use your own trainer class, you have to pass; <pre><code>--training_class examples.MyBinaryClassificationModel\n</code></pre></p> <p>In above example, framework will import <code>MyBinaryClassificationModel</code> from <code>examples</code> and will fit the model which is already in <code>MyBinaryClassificationModel</code>. Model has to be assigned to <code>self.model</code> in the class.</p> <p>In the training class there are some rules;</p> <ul> <li>under training class, <code>mlp.BaseModel</code> will be used as sub-class.</li> <li>there are some abstractmethod will be used within trainer class that are coming from <code>mlp.BaseModel</code></li> <li><code>__init__(self, params: mlp.Params)</code>: initialize your training class and don't forget to add aguments as <code>params</code> there. <code>params</code> will be coming from trainer config path which you have to pass from terminal in <code>--tariner_config_path</code></li> </ul> <p><pre><code>from mlp import BaseModel, Params\n\n\nclass MyBinaryClassificationModel(BaseModel):\n    def __init__(self, params: Params):\n        self.params = params\n        ....\n</code></pre> - Another abstractmethod that is coming from <code>mlp.BaseModel</code>, is <code>train</code>. In order to train your model trainer attribute will be needed. This method will need arguments <code>dataset: BaseData.data_type</code>. you can use <code>dataset</code> in your model.  How to use data set will be in dataset section.</p> <p><pre><code>from mlp import BaseModel, Params\n\n\nclass MyBinaryClassificationModel(BaseModel):\n    def __init__(self, params: Params):\n        self.params = params\n        ....\n\n    def train(self, dataset: BaseData.data_type) -&gt; None:\n        ....\n        ....\n</code></pre> - Model has to be built within trainer class. It has to be assigned to <code>self.model</code>. Where ever you need Network <code>self.model</code> attribute will be called. <code>self.model</code> has to be a <code>keras.Model</code>. In below example, model will be created at <code>__init__</code> from <code>buildv2</code>;</p> <pre><code>from mlp import BaseModel, Params\n\n\nclass MyBinaryClassificationModel(BaseModel):\n    def __init__(self, params: Params):\n        self.params = params\n        self.model = buildv2()\n\n    def buildv2(self) -&gt; keras.Model:\n        ....\n        return keras.Model()\n</code></pre> <p>Let's take a look at the below example;</p> <pre><code>class MyBinaryClassificationModel(BaseModel):\n    def __init__(self, params: Params):\n        self.params = params\n        self.model = self.buildv1()\n\n    def metrics(self):\n        return [Metrics.train_epoch_metrics(metric) for metric in self.params.metrics]\n\n    def buildv1(self) -&gt; keras.Model:\n        h_units = BaseModel.cal_hidden_layer_of_units(\n            self.params.h_layers, self.params.units\n        )\n        _input = layers.Input(\n            name=f\"{self.params.model_type}_{self.params.name}_input\",\n            shape=(self.params.input_size,),\n        )\n        _hidden = layers.BatchNormalization()(_input)\n        for _unit in h_units:\n            _hidden = layers.Dense(\n                _unit,\n                activation=BaseModel.decision_of_activation(self.params.activation),\n                use_bias=False,\n            )(_hidden)\n            _hidden = layers.Dropout(self.params.dropout)(_hidden)\n        output = layers.Dense(\n            self.params.output_size,\n            name=\"output\",\n            activation=BaseModel.decision_of_activation(self.params.activation_output),\n            use_bias=self.params.use_bias,\n            kernel_regularizer=regularizers.l1_l2(l1=self.params.l1, l2=self.params.l2),\n        )(_hidden)\n        model = keras.Model(inputs=_input, outputs=output)\n        model.compile(\n            loss=self.params.loss,\n            optimizer=self.optimizer(self.params.optimizer, self.params.lr),\n            metrics=self.metrics(),\n        )\n        return model\n</code></pre> <p>Model will be trained by <code>MyBinaryClassificationModel</code>. As you see that Network has been created by <code>buildv1</code>. You can use this template to build your own network.</p> <p>Another approach is that you can use your build-in classes to create your own network. In below example, network will be built by using <code>mlp.Network</code> class. You don't needd to use a trainer class to call <code>mlp.Network</code>.  By only passing <code>trainer_config_path</code> model will create network by using <code>mlp.Network</code>.</p> <pre><code>class MyBinaryClassificationModelV2(BaseModel):\n    def __init__(self, params: Params):\n        self.params = params\n        self.model = self.build()\n\n    def build(self):\n        from mlp import Network\n        network = Network(params=self.params)\n        network.build_network_from_config()\n        return network.model\n\n    def train(self, dataset: BaseData.data_type):\n        self.model.fit(\n            # (True (train-val split), True (y variable available for this data), x (we are taking INPUT variables))\n            x=dataset[(True, True, \"x\")],\n            # (True (train-val split), True (y variable available for this data), x (we are taking TARGET variable))\n            y=dataset[(True, True, \"y\")],\n            # (True (train-val split), True (y variable available for this data),\n            # validation_data (we are taking validation data tuple(x, y) ))\n            validation_data=dataset[(True, True, \"validation_data\")],\n            batch_size=self.params.batch_size,\n            epochs=self.params.epochs,\n        )\n</code></pre>"},{"location":"train/#train-from-configuration","title":"Train from Configuration","text":"<p>You can directly use built-in modules to create Network</p>"},{"location":"tune/","title":"HyperParameter Tuning","text":""},{"location":"tune/#tuner-class","title":"tuner Class","text":""},{"location":"tune/#tune-from-configuration","title":"Tune from Configuration","text":""}]}